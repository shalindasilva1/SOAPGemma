{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-22T12:06:43.627340Z",
     "start_time": "2025-05-22T12:06:43.623097Z"
    }
   },
   "source": [
    "import torch\n",
    "import bitsandbytes as bnb\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from datasets import Dataset\n",
    "\n",
    "DATA_PATH = Path(\"../data\")\n",
    "OMI_PATH_processed = DATA_PATH / \"processed\" / \"omi-health\"\n",
    "OMI_PATH_raw = DATA_PATH / \"raw\" / \"omi-health\"\n",
    "MODEL_PATH =  Path(\"../models\")\n",
    "\n",
    "print(f\"PyTorch CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device count: {torch.cuda.device_count()}\")\n",
    "    print(f\"Current CUDA device name: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "print(f\"Bitsandbytes version: {bnb.__version__}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch CUDA available: True\n",
      "CUDA device count: 1\n",
      "Current CUDA device name: NVIDIA RTX A4000\n",
      "Bitsandbytes version: 0.45.5\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T12:08:39.996016Z",
     "start_time": "2025-05-22T12:06:44.523429Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_id = \"google/txgemma-2b-predict\"\n",
    "\n",
    "# Use 4-bit quantization to reduce memory usage\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map={\"\":0},\n",
    "    torch_dtype=\"auto\",\n",
    "    attn_implementation=\"eager\",\n",
    ")"
   ],
   "id": "80e9405da260a5a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\khal6952\\AppData\\Local\\miniconda3\\envs\\soap\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\khal6952\\.cache\\huggingface\\hub\\models--google--txgemma-2b-predict. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Downloading shards: 100%|██████████| 3/3 [01:45<00:00, 35.28s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.63s/it]\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T12:09:55.933711Z",
     "start_time": "2025-05-22T12:09:54.685256Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "train_df = pd.read_csv(OMI_PATH_processed / \"train_v1.csv\")\n",
    "train_df.head()"
   ],
   "id": "b153b9a487c93b0e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                            dialogue  \\\n",
       "0  Doctor: Hello, how can I help you today?\\nPati...   \n",
       "1  Doctor: Hello, what brings you in today?\\nPati...   \n",
       "2  Doctor: Hello, how can I help you today?\\nPati...   \n",
       "3  Doctor: Hello, Patient D. How are you feeling ...   \n",
       "4  Doctor: Hello, I see that you have a history o...   \n",
       "\n",
       "                                                soap  \\\n",
       "0  S: The patient's mother reports that her 13-ye...   \n",
       "1  S: The patient, a 21-month-old male, presented...   \n",
       "2  S: Patient reports experiencing fatigue, night...   \n",
       "3  S: Patient D, a 60-year-old African American m...   \n",
       "4  S: The patient, a married woman with a 7-year ...   \n",
       "\n",
       "                                              prompt  \\\n",
       "0  Create a Medical SOAP note summary from the di...   \n",
       "1  Create a Medical SOAP note summary from the di...   \n",
       "2  Create a Medical SOAP note summary from the di...   \n",
       "3    Create a medical SOAP summary of this dialogue.   \n",
       "4  Create a Medical SOAP note summary from the di...   \n",
       "\n",
       "                                            messages  \\\n",
       "0  [{'role': 'system', 'content': 'You are an exp...   \n",
       "1  [{'role': 'system', 'content': 'You are an exp...   \n",
       "2  [{'role': 'system', 'content': 'You are an exp...   \n",
       "3  [{'role': 'system', 'content': 'You are an exp...   \n",
       "4  [{'role': 'system', 'content': 'You are an exp...   \n",
       "\n",
       "                                   messages_nosystem  \\\n",
       "0  [{'role': 'user', 'content': \"You are an exper...   \n",
       "1  [{'role': 'user', 'content': \"You are an exper...   \n",
       "2  [{'role': 'user', 'content': \"You are an exper...   \n",
       "3  [{'role': 'user', 'content': \"You are an exper...   \n",
       "4  [{'role': 'user', 'content': \"You are an exper...   \n",
       "\n",
       "                                          event_tags  \n",
       "0                              ['(After the tests)']  \n",
       "1  ['[After the tests]', '[After 3 weeks of thera...  \n",
       "2                                                 []  \n",
       "3                                                 []  \n",
       "4                                                 []  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dialogue</th>\n",
       "      <th>soap</th>\n",
       "      <th>prompt</th>\n",
       "      <th>messages</th>\n",
       "      <th>messages_nosystem</th>\n",
       "      <th>event_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Doctor: Hello, how can I help you today?\\nPati...</td>\n",
       "      <td>S: The patient's mother reports that her 13-ye...</td>\n",
       "      <td>Create a Medical SOAP note summary from the di...</td>\n",
       "      <td>[{'role': 'system', 'content': 'You are an exp...</td>\n",
       "      <td>[{'role': 'user', 'content': \"You are an exper...</td>\n",
       "      <td>['(After the tests)']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Doctor: Hello, what brings you in today?\\nPati...</td>\n",
       "      <td>S: The patient, a 21-month-old male, presented...</td>\n",
       "      <td>Create a Medical SOAP note summary from the di...</td>\n",
       "      <td>[{'role': 'system', 'content': 'You are an exp...</td>\n",
       "      <td>[{'role': 'user', 'content': \"You are an exper...</td>\n",
       "      <td>['[After the tests]', '[After 3 weeks of thera...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Doctor: Hello, how can I help you today?\\nPati...</td>\n",
       "      <td>S: Patient reports experiencing fatigue, night...</td>\n",
       "      <td>Create a Medical SOAP note summary from the di...</td>\n",
       "      <td>[{'role': 'system', 'content': 'You are an exp...</td>\n",
       "      <td>[{'role': 'user', 'content': \"You are an exper...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Doctor: Hello, Patient D. How are you feeling ...</td>\n",
       "      <td>S: Patient D, a 60-year-old African American m...</td>\n",
       "      <td>Create a medical SOAP summary of this dialogue.</td>\n",
       "      <td>[{'role': 'system', 'content': 'You are an exp...</td>\n",
       "      <td>[{'role': 'user', 'content': \"You are an exper...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Doctor: Hello, I see that you have a history o...</td>\n",
       "      <td>S: The patient, a married woman with a 7-year ...</td>\n",
       "      <td>Create a Medical SOAP note summary from the di...</td>\n",
       "      <td>[{'role': 'system', 'content': 'You are an exp...</td>\n",
       "      <td>[{'role': 'user', 'content': \"You are an exper...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T12:10:04.091851Z",
     "start_time": "2025-05-22T12:10:03.336Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset = Dataset.from_pandas(train_df)\n",
    "dataset = dataset.rename_column('soap', 'completion')\n",
    "cols_to_drop = ['prompt', 'messages', 'messages_nosystem']\n",
    "dataset = dataset.remove_columns(cols_to_drop)\n",
    "dataset"
   ],
   "id": "e5b5b9a167e66650",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['dialogue', 'completion', 'event_tags'],\n",
       "    num_rows: 9250\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T12:10:04.244203Z",
     "start_time": "2025-05-22T12:10:04.139225Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from peft import LoraConfig\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"o_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],\n",
    ")"
   ],
   "id": "5832f3407ed2a951",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T12:10:04.561012Z",
     "start_time": "2025-05-22T12:10:04.298282Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from peft import prepare_model_for_kbit_training, get_peft_model\n",
    "\n",
    "# Preprocess quantized model for training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# Create PeftModel from quantized model and configuration\n",
    "model = get_peft_model(model, lora_config)"
   ],
   "id": "732a05aa0b42cf5",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T12:10:04.663535Z",
     "start_time": "2025-05-22T12:10:04.660093Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def format_dialogue_for_soap_synthesis_v2(data):\n",
    "    dialogue = data[\"dialogue\"]\n",
    "    soap = data[\"completion\"]\n",
    "    return f\"dialogue: {dialogue}<soap_start> soap_note:{soap} <eos>\""
   ],
   "id": "4c49074f58bd3466",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T12:10:23.110583Z",
     "start_time": "2025-05-22T12:10:04.749906Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import transformers\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    args=SFTConfig(\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=4,\n",
    "        warmup_steps=8,\n",
    "        max_steps=100,\n",
    "        learning_rate=5e-4,\n",
    "        fp16=True,\n",
    "        logging_steps=5,\n",
    "        max_seq_length=2048,\n",
    "        output_dir=\"/content/outputs\",\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        report_to=\"none\",\n",
    "    ),\n",
    "    peft_config=lora_config,\n",
    "    formatting_func=format_dialogue_for_soap_synthesis_v2,\n",
    ")"
   ],
   "id": "77f8053376bf1891",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Applying formatting function to train dataset: 100%|██████████| 9250/9250 [00:00<00:00, 18506.11 examples/s]\n",
      "Converting train dataset to ChatML: 100%|██████████| 9250/9250 [00:00<00:00, 21536.25 examples/s]\n",
      "Adding EOS to train dataset: 100%|██████████| 9250/9250 [00:00<00:00, 12920.37 examples/s]\n",
      "Tokenizing train dataset: 100%|██████████| 9250/9250 [00:15<00:00, 602.67 examples/s]\n",
      "Truncating train dataset: 100%|██████████| 9250/9250 [00:00<00:00, 170526.39 examples/s]\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T12:21:56.487585Z",
     "start_time": "2025-05-22T12:10:23.149473Z"
    }
   },
   "cell_type": "code",
   "source": "trainer.train()",
   "id": "ebb836595cd5e59b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "C:\\Users\\khal6952\\AppData\\Local\\miniconda3\\envs\\soap\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 11:26, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>14.171600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>6.748300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>3.597900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.658000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>2.322500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.066300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>1.880100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.841100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>1.803000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.826200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>1.752700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.703400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>1.598400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.691000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>1.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.622300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>1.652900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.660200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>1.529000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.679300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=100, training_loss=2.77772243976593, metrics={'train_runtime': 693.122, 'train_samples_per_second': 0.577, 'train_steps_per_second': 0.144, 'total_flos': 4708887651749376.0, 'train_loss': 2.77772243976593})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T12:21:57.219663Z",
     "start_time": "2025-05-22T12:21:56.554149Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Save the fine-tuned model\n",
    "model.save_pretrained(MODEL_PATH / \"SOAPgemma_v1\")\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer.save_pretrained(MODEL_PATH / \"SOAPgemma_v1\")\n",
    "\n",
    "print(f\"Model and tokenizer saved to {MODEL_PATH / 'SOAPgemma_v1'}\")"
   ],
   "id": "768560d391533498",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer saved to ..\\models\\SOAPgemma_v1\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T12:21:57.264212Z",
     "start_time": "2025-05-22T12:21:57.260400Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_soap_note(dialogue, model, tokenizer, device=\"cuda:0\"):\n",
    "    \"\"\"Generates a SOAP note from a given dialogue.\"\"\"\n",
    "\n",
    "    # MODIFIED: Align with the training prompt structure to guide generation\n",
    "    input_text = f\"dialogue: {dialogue}<soap_start> soap_note:\"\n",
    "\n",
    "    # Tokenize the input\n",
    "    inputs = tokenizer.encode_plus(\n",
    "        input_text,\n",
    "        return_tensors=\"pt\",\n",
    "        # Ensure tokenizer doesn't add EOS token here if the model adds it during generation\n",
    "        add_special_tokens=True # Or False, depending on tokenizer and model behavior with this specific prompt\n",
    "    ).to(device)\n",
    "\n",
    "    # Generate the SOAP note\n",
    "    outputs = model.generate(\n",
    "        inputs.input_ids,\n",
    "        # MODIFIED: Use max_new_tokens to control the length of the *generated* text\n",
    "        max_new_tokens=512,  # Adjust as needed for typical SOAP note length\n",
    "        num_beams=4,\n",
    "        temperature=0.7,\n",
    "        # IMPORTANT: Add pad_token_id if not set in model config, especially for open-ended generation\n",
    "        pad_token_id=tokenizer.eos_token_id, # Or tokenizer.pad_token_id if different and model expects it\n",
    "        # Consider adding an early stopping criterion or specific stop sequences if needed\n",
    "        # eos_token_id=tokenizer.eos_token_id # Ensure model stops at <eos>\n",
    "    )\n",
    "\n",
    "    # Decode the output\n",
    "    # MODIFIED: Slice the output to remove the input prompt text\n",
    "    # The generated tokens start *after* the input_ids length\n",
    "    generated_ids = outputs[0][inputs.input_ids.shape[1]:]\n",
    "    soap_note = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "    return soap_note"
   ],
   "id": "c4b075eb2a668dc2",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T12:22:29.749463Z",
     "start_time": "2025-05-22T12:21:57.309170Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sample_dialogue = train_df.iloc[0,0]\n",
    "generated_note = generate_soap_note(sample_dialogue, model, tokenizer)\n",
    "print(\"Generated SOAP Note:\")\n",
    "print(generated_note)"
   ],
   "id": "88ca60c10b2641b6",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\khal6952\\AppData\\Local\\miniconda3\\envs\\soap\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "C:\\Users\\khal6952\\AppData\\Local\\miniconda3\\envs\\soap\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "C:\\Users\\khal6952\\AppData\\Local\\miniconda3\\envs\\soap\\Lib\\site-packages\\torch\\utils\\checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated SOAP Note:\n",
      "S: The patient reports that their son has been experiencing mild to moderate speech and developmental delay, diagnosed with attention deficit disorder at age 13. The patient also notes that the son has mild to moderate hypotonia.\n",
      "O: MRI results indicate no structural brain anomalies. Physical examination revealed facial features like retrognathia, mild hypertelorism, a thin upper lip, and a sandal gap in both feet. Genetic testing revealed a de novo frameshift variant in Chr1 (GRCH3:37]) located more than 400 codons upstream from the canonical termination codon.\n",
      "A: The primary diagnosis is attention deficit disorder, likely contributing to the son's speech, developmental delay, and attention deficit. The presence of a de novo frameshift variant in Chr1 (GRCH3:37]) suggests a genetic component in the son's condition.\n",
      "P: The management plan includes regular visits with a speech therapist, an occupational therapist, and a psychologist to address the son's developmental and attention deficit disorder needs. Regular check-ups with me to monitor growth and overall health. Referral to a speech therapist and an occupational therapist to address speech and developmental delays. Regular monitoring of the son's speech and developmental progress through regular visits and regular check-ups. \n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T12:52:59.858738Z",
     "start_time": "2025-05-22T12:52:59.854007Z"
    }
   },
   "cell_type": "code",
   "source": "train_df.iloc[0,1]",
   "id": "6d8803cb0526ba28",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"S: The patient's mother reports that her 13-year-old son has mild to moderate speech and developmental delays and has been diagnosed with attention deficit disorder. She denies any issues with muscle tone or hypotonia. The patient also exhibits certain physical characteristics, including retrognathia, mild hypertelorism, an elongated philtrum, thin upper lip, broad and short hands, mild syndactyly of the second and third toes, and a sandal gap in both feet.\\nO: An MRI of the brain showed no structural anomalies. Whole Exome Sequencing (WES) revealed a de novo frameshift variant Chr1(GRCh37):g.244217335del, NM_205768.2(ZBTB18):c.259del(p.(Leu87Cysfs*21)), indicating a premature termination codon located more than 400 codons upstream of the canonical termination codon.\\nA: The primary diagnosis is a genetic disorder associated with the identified frameshift mutation, which likely contributes to the patient's speech and developmental delays and attention deficit disorder. The physical characteristics and genetic findings suggest a specific syndrome, which needs further correlation with clinical findings and genetic counseling.\\nP: The management plan includes regular follow-up visits with a speech and language therapist, an occupational therapist, and a psychologist to support the patient's developmental needs and address his attention deficit disorder. Regular medical check-ups will monitor his growth and overall health. Genetic counseling for the family is also recommended to discuss the implications of the genetic findings and potential familial inheritance.\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
